{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFCa1bF9Oy0S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMbYVj68Ozfa"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize(256),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fri46opO0_I"
      },
      "outputs": [],
      "source": [
        "def position_embedding_layer(num_patches, embedding_dimension, batch_size):\n",
        "    result = torch.ones(num_patches + 1, embedding_dimension)  # Add 1 for the class token\n",
        "    for i in range(num_patches + 1):  # Add 1 for the class token\n",
        "        for j in range(embedding_dimension):\n",
        "            if j % 2 == 0:\n",
        "                result[i][j] = (np.sin(i / (10000 ** (j / embedding_dimension))))\n",
        "            else:\n",
        "                result[i][j] = np.cos(i / (10000 ** ((j - 1) / embedding_dimension)))\n",
        "    result = result.unsqueeze(0).repeat(3, 1, 1)\n",
        "    result = result.unsqueeze(0).repeat(batch_size,1,1,1)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OFDw9GiO35v"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pPwCm4sBtTL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def patchify(images, num_of_patches):\n",
        "    batch_size, channels, height, width = images.shape\n",
        "\n",
        "    assert height == width, \"Patchify method is implemented for square images only\"\n",
        "\n",
        "    height_per_patch = height // num_of_patches\n",
        "    width_per_patch = width // num_of_patches\n",
        "\n",
        "    patches = torch.zeros(batch_size, num_of_patches**2, channels, height_per_patch, width_per_patch)\n",
        "\n",
        "    for idx, image in enumerate(images):\n",
        "        for i in range(num_of_patches):\n",
        "            for j in range(num_of_patches):\n",
        "                patch = image[\n",
        "                    :,\n",
        "                    i * height_per_patch : (i + 1) * height_per_patch,\n",
        "                    j * width_per_patch : (j + 1) * width_per_patch,\n",
        "                ]\n",
        "                patches[idx, i * num_of_patches + j] = patch\n",
        "\n",
        "    return patches\n",
        "\n",
        "class Projection_Layer(nn.Module):\n",
        "    def __init__(self, num_patches,patch_size,in_channels, embed_size):\n",
        "        super(Projection_Layer, self).__init__()\n",
        "\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self.layer_norm_1 = nn.LayerNorm(patch_size*patch_size)\n",
        "        self.embed_layer = nn.Linear(patch_size*patch_size,embed_size//3)\n",
        "        self.layer_norm_2 = nn.LayerNorm(embed_size//3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b,_,c,_,_ = x.shape\n",
        "        x = x.view(b,c,self.num_patches,-1)\n",
        "        x = self.layer_norm_1(x)\n",
        "        x = self.embed_layer(x)\n",
        "        x = self.layer_norm_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self,num_heads, embed_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        eff_embed_size = embed_size//num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.Q_matrix = nn.Linear(eff_embed_size,eff_embed_size)\n",
        "        self.K_matrix = nn.Linear(eff_embed_size,eff_embed_size)\n",
        "        self.V_matrix = nn.Linear(eff_embed_size,eff_embed_size)\n",
        "\n",
        "        self.Q_final = nn.Sequential(nn.Linear(3 * eff_embed_size, 3* eff_embed_size),nn.GELU(),nn.Linear(3 * eff_embed_size, 3* eff_embed_size))\n",
        "        self.K_final = nn.Sequential(nn.Linear(3 * eff_embed_size, 3* eff_embed_size),nn.GELU(),nn.Linear(3 * eff_embed_size, 3* eff_embed_size))\n",
        "        self.V_final = nn.Sequential(nn.Linear(3 * eff_embed_size, 3* eff_embed_size),nn.GELU(),nn.Linear(3 * eff_embed_size, 3* eff_embed_size))\n",
        "\n",
        "        self.temperature = eff_embed_size**0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs,c,n_1,embed_dim = x.shape\n",
        "        x = x.view(bs,c,self.num_heads,n_1,embed_dim//self.num_heads)  ## B, head, 256, 192//head\n",
        "        q = self.Q_matrix(x)\n",
        "        k = self.K_matrix(x)\n",
        "        v = self.V_matrix(x)\n",
        "\n",
        "        q1 = q[:,0,:,:]\n",
        "        q2 = q[:,1,:,:]\n",
        "        q3 = q[:,2,:,:]\n",
        "\n",
        "        k1 = k[:,0,:,:]\n",
        "        k2 = k[:,1,:,:]\n",
        "        k3 = k[:,2,:,:]\n",
        "\n",
        "        v1 = v[:,0,:,:]\n",
        "        v2 = v[:,1,:,:]\n",
        "        v3 = v[:,2,:,:]\n",
        "\n",
        "        # Concatenate queries, keys, and values\n",
        "        q_concat = torch.cat((q1, q2, q3), dim=-1)\n",
        "        k_concat = torch.cat((k1, k2, k3), dim=-1)\n",
        "        v_concat = torch.cat((v1, v2, v3), dim=-1)\n",
        "\n",
        "        q_final = self.Q_final(q_concat)\n",
        "        k_final = self.K_final(k_concat)\n",
        "        v_final = self.V_final(v_concat)\n",
        "\n",
        "        attention = nn.Softmax(dim=-1)(torch.matmul(q_final, k_final.transpose(-1, -2))) / self.temperature\n",
        "        x = torch.matmul(attention, v_final)\n",
        "        x = x.view(bs, n_1, embed_dim * 3)\n",
        "        #x = x.unsqueeze(1).expand(-1, 3, -1, -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Transformer_Block(nn.Module):\n",
        "    def __init__(self, num_heads,embed_size,hidden_dim,dropout):\n",
        "        super(Transformer_Block, self).__init__()\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.attn = Attention(num_heads, embed_size)\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.LayerNorm(embed_size),\n",
        "            nn.Linear(embed_size, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, embed_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        b,c,n_1,e = x.shape\n",
        "        x = self.norm(x)\n",
        "        x = self.attn(x).view(b,c,n_1,e) + x\n",
        "        x = self.MLP(x) + x\n",
        "\n",
        "        return x\n",
        "\n",
        "class Vision_Transformer(nn.Module):\n",
        "    def __init__(self, image_size=256, in_channels=3, patch_size=16, embed_size=192, hidden_dim=768, num_heads=8, num_layers=12, dropout=0.01, num_of_patches=4):\n",
        "        super(Vision_Transformer, self).__init__()\n",
        "\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.projection_layer = Projection_Layer(self.num_patches, patch_size,in_channels, embed_size)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1,1, embed_size//3))\n",
        "        # Using position embedding layer here is assumed\n",
        "        self.pos_emb = position_embedding_layer(self.num_patches, embed_size//3, 1)  # Using position_embedding_layer\n",
        "\n",
        "        self.layers = nn.Sequential(*[Transformer_Block(num_heads, embed_size//3, hidden_dim, dropout)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "        self.clf_head = nn.Linear(embed_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, _, _, _ = x.shape\n",
        "        x = patchify(x, int(self.num_patches ** 0.5))  # Patchify images\n",
        "        x = self.projection_layer(x)  # Flatten patches and project\n",
        "\n",
        "        cls_token = self.cls_token.expand(bs,3, -1, -1)  # Broadcasting\n",
        "        x = torch.cat([cls_token, x], dim=2)\n",
        "\n",
        "        x = x + self.pos_emb  # Adding position embeddings\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x=x.view(bs,self.num_patches+1,-1)\n",
        "        x = self.clf_head(x[:, 0, :])  # Output classification\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBV0JvfoO9Of"
      },
      "outputs": [],
      "source": [
        "net = Vision_Transformer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUjIp5pOO_f4"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=2e-4, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v1JfigbPBGI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "for epoch in range(20):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in tqdm(enumerate(trainloader, 0)):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0], data[1]\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkaTJRVhPEiP"
      },
      "outputs": [],
      "source": [
        "net.eval()\n",
        "\n",
        "# Initialize variables to track test loss and accuracy\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Disable gradient computation for test phase\n",
        "with torch.no_grad():\n",
        "    for i, data in tqdm(enumerate(testloader, 0)):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0], data[1]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Accumulate the test loss\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Compute the predicted labels\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Update the total and correct predictions count\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f'Test Loss: {test_loss / len(testloader):.3f}')\n",
        "print(f'Test Accuracy: {(100 * correct / total):.2f}%')\n",
        "\n",
        "# Set the model back to training mode\n",
        "net.train()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}